Sentiment Analysis with Recursive Neural Network
*************************************************

.. currentmodule:: chainer

0. Introduction
================

In this tutorial, we will use the Recursive Neural Network to analyze sentiment in sentences.

Sentiment analysis is one of the major tasks of Natural Language Processing (NLP),
which identifies writersâ€™ sentiments in sentences. When expressing sentiment,
we basically uses labels whether it is positive or negative. For example,
in the case of the dataset used this time, emotions are expressed in 5 classes
like 1(very negative), 2(negative), 3(neutral), 4(positive), 5(very positive).

.. figure:: ../../image/sentiment/sentiment-treebank.png
    :scale: 100%

    cited from [1]

Sentiment analysis is implemented with Recursive Neural Networks. Recursive Neural
Networks are recurrent neural nets with a tree structure. NLP often expresses
sentences in a tree structure, Recursive Neural Networks are often used in NLP.
First, we explain the training method of Recursive Neural Networks without
mini-batch processing. After that, as an advanced story, we explain the training
method of a mini-batched Recursive Neural Network.

In this tutorial, we will learn the following:

#. What is a Recursive Neural Network?
#. Implementation of sentiment analysis by Recursive Neural Networks using Chainer

    * Training method of a Recursive Neural Network without mini-batches
    * Training method of a mini-batched Recursive Neural Network

1. What is a Recursive Neural Network? 
=======================================

A Recursive Neural Network is a Recurrent Neural Networks that extends to
a tree structure. As both networks are often written as RNN, we need to be
careful which one we are referring to. RNN usually refers to the Recurrent
Neural Networks, but in natural language processing it sometimes
refers to Recursive Neural Networks.

Recursive Neural Networks use a tree structure with a fixed number of branches.
In the case of a binary tree, the hidden state vector of the current node is
computed from the hidden state vectors of the left and right child nodes,
as follows:

.. math::
    {\bf h}_P = a \left( {\bf W} 
    \left[ \begin{array}{l}
        {\bf h}_L \\
        {\bf h}_R \\
    \end{array} \right]
    + {\bf b} \right)

.. image:: ../../image/sentiment/rnn.png

This operation is sequentially calculated from the leaf nodes toward the root node.
Recursive Neural Networks are expected to express relationships between long-distance
elements compared to Recurrent Neural Networks, because the depth is enough with
:math:`log_2(T)` on average if the element count is :math:`T`.

2. Implementation of sentiment analysis by Recursive Neural Networks
=====================================================================

2.1 Preparation of training data
---------------------------------

In this tutorial, we will use the training data which are preprocessed by 
`chainer/examples/sentiment/download.py <https://github.com/chainer/chainer/blob/master/examples/sentiment/download.py>`_.

.. literalinclude:: ../../../examples/sentiment/download.py
   :language: python
   :caption: download.py

Let's run the following codes, download the necessary training data and unzip it.

.. code-block:: console

    $ ./download.py

Let's execute the following command and check if the training data have been prepared.

.. code-block:: console

    $ ls trees
    dev.txt  test.txt  train.txt

Let's look at the first line of test.txt and see how each sample is written.

.. code-block:: console

    $ head trees/dev.txt -n1
    (3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))

As displayed above, each sample is defined by a tree structure called as "parse tree"[2].  These parse trees are generated by Stanford Parser[3].
The tree structure is recursively defined as ``(value, node)``, and the class label for node is value.
The class labels represent 1(very negative), 2(negative), 3(neutral), 4(positive), and 5(very positive), respectively.

The representation of one sample is shown below.

.. figure:: ../../image/sentiment/sentiment-treebank.png
    :scale: 100%

    cited from [1]

2.2 Setting parameters
---------------------------------

Here we set the parameters for training.

* ``n_epoch``: Epoch number. How many times we pass through the whole training data.
* ``n_units``: Number of units. How many hidden state vectors each Recursive Neural Network node has.
* ``batchsize``: Batch size. How many training data we will input as a block when updating parameters.
* ``n_label``: Number of labels. Number of classes to be identified. Since there are 5 labels this time, ``5``.
* ``epoch_per_eval``: How often to perform validation.

.. literalinclude:: ../../../examples/sentiment/train_sentiment.py
   :language: python
   :start-after: parser.parse_args
   :end-before: if args.test
   :caption: train_sentiment.py
   :dedent: 4

2.3 Preparing the iterator
----------------------------

Let's read the dataset used for training, validation, test and create an Iterator.

First, we convert each sample represented by ``str`` type to a tree structure data represented by a ``dictionary`` type.
We will tokenize the string with ``read_corpus`` implemented by the parser ``SexpParser``.
After that, we convert each tokenized sample to a tree structure data  by ``convert_tree``. In this way,
it is possible to express a label as ``int``, a node as a two-element ``tuple``, and a tree structure
as a ``dictionary``, making it a more manageable data structure than the original string.

.. literalinclude:: ../../../examples/sentiment/data.py
   :language: python
   :caption: data.py

Let's use ``read_corpus()`` and ``convert_tree()`` to create an iterator.

.. literalinclude:: ../../../examples/sentiment/train_sentiment.py
   :language: python
   :start-after: max_size = None
   :end-before: model = RecursiveNet
   :caption: train_sentiment.py
   :dedent: 4

2.4 Preparing the model
------------------------

We traverse each node of the tree structure data by ``traverse`` and calculate the loss ``loss``
of the whole tree. The implementation of ``traverse`` is a recursive call, which will traverse child nodes
in turn. (It is a common implementation when treating tree structure data!)

First, we calculate the hidden state vector ``v``. In the case of a leaf node, we obtain a hidden state vector
stored in ``embed`` by ``model.leaf(word)`` from word id ``word``. In the case of an intermediate node,
the hidden vector is calculated with the hidden state vector ``left`` and ``right`` of the child nodes by
``v = model.node(left, right)``.
``loss += F.softmax_cross_entropy(y, t)`` adds the loss of the current node to the loss of the child node,
then returns loss to the parent node by ``return loss, v``.
After the line ``loss += F.softmax_cross_entropy(y, t)``, there are some lines for logging accuracy and etc.
But it is not necessary for the model definition itself.

Let's define the network.

.. literalinclude:: ../../../examples/sentiment/train_sentiment.py
   :language: python
   :pyobject: RecursiveNet
   :caption: train_sentiment.py

One attention to the implementation of ``__call__``.
``x`` passed to ``__call__`` is mini-batched input data and contains samples ``s_n`` like ``[s_1, s_2, ..., s_N]``.
In a network such as Convolutional Network used for image recognition, it is possible to perform parallel
calculation collectively for mini batch ``x``. However, in the case of a tree-structured network like this one,
it is difficult to compute parallel because of the following reasons.

* Data length varies depending on samples.
* The order of calculation for each sample is different.

So, the implementation is to calculate each sample and finally summarize the results.

.. literalinclude:: ../../../examples/sentiment/train_sentiment.py
   :language: python
   :start-after: for tree in data.read_corpus('trees/test.txt'
   :end-before: def _convert(batch, _)
   :caption: train_sentiment.py
   :dedent: 4
 
.. note::
    Actually, we can perform parallel calculations for mini batches in Recursive Neural Networks by using stacks.
    Since it is published in the latter part of notebook as (Advanced), please refer to it.


2.5 Preparation and training of Updater and Trainer
-----------------------------------------------------

As usual, we define an updater and a trainer to train the model.
This time, I do not use :class:`~chainer.links.Classifier` and calculate the accuracy ``accuracy`` by myself.
You can easily implement it using :class:`~chainer.training.extensions.MicroAverage`.

.. literalinclude:: ../../../examples/sentiment/train_sentiment.py
   :language: python
   :start-after: optimizer.add_hook
   :end-before: print('Test evaluation')
   :caption: train_sentiment.py
   :dedent: 4

2.6 Checking the performance with test data
--------------------------------------------

.. literalinclude:: ../../../examples/sentiment/train_sentiment.py
   :language: python
   :pyobject: evaluate
   :caption: train_sentiment.py

.. literalinclude:: ../../../examples/sentiment/train_sentiment.py
   :language: python
   :start-after: trainer.run()
   :end-before: __main__
   :caption: train_sentiment.py
   :dedent: 4

2.7 Start training and check the performance
---------------------------------------------

.. code-block:: console

    $ pwd
    /root2chainer/chainer/examples/sentiment
    $ python train_sentiment.py --epoch 4 --test  # run by test mode. If we want to use all data, remove "--test".
    epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time
    1           1573.94                           0.202703                                 1.06574       
    2           522.063     617.106               0.617117       0.415301                  1.70406       
    3           448.05                            0.693694                                 2.19765       
    4           410.08      544.221               0.722973       0.448087                  2.82558       
    Test evaluation
     Node accuracy: 46.47 %% (145/312)
     Root accuracy: 40.00 %% (4/10)

3. (Advanced) Mini-batching in Recursive Neural Networks[4]
============================================================

Recursive Neural Networks have difficulty computing mini-batched data in parallel because of the following reasons.

* Data length varies depending on samples.
* Calculation order of nodes varies depending on samples.

However, using the stack, we can train Recursive Neural Networks in mini-batched parallel computing.

3.1 Preparation of Dataset, Iterator
-------------------------------------

First, we convert recursive calculation to serial calculation using a stack.
To serialize the calculation, we assign a number in "returning order" to each node of the tree in dataset.

.. figure:: ../../image/sentiment/returning-order.png
    :scale: 50%

The returning order is a procedure of numbering nodes of a tree.
As shown in the above image,
all child nodes have smaller numbers than the parents' ones.
If we trace nodes in descending order, we can visit every child node before its parent node.

To number the nodes in returning order, ``linearize_tree``
creates a dictionary which has following keys.

* ``dests``: All parent node indexes.
* ``lefts``: Left node indices for all parent nodes. ``dests[i]``'s left child node is ``lefts[i]``.
* ``rights``: Right node indices for all parent nodes. ``dests[i]``'s right child node is ``rights[i]``.
* ``labels``: Labels for all parent nodes. ``dests[i]``'s label is ``labels[i]``.
* ``words``: Words of all leaf nodes. The ``i``-th leaf node's word is ``words[i]``.
* ``leaf_labels``: Labels for all leaf nodes. The ``i``-th leaf node's label is ``leaf_labels[i]``.

.. literalinclude:: ../../../examples/sentiment/train_recursive_minibatch.py
   :language: python
   :pyobject: linearize_tree
   :caption: train_recursive_minibatch.py

.. literalinclude:: ../../../examples/sentiment/train_recursive_minibatch.py
   :language: python
   :start-after: parser.parse_args()
   :end-before: model = ThinStackRecursiveNet
   :caption: train_recursive_minibatch.py
   :dedent: 4

.. code-block:: console

    >>> print(test_data[0])
    {'lefts': array([0, 2, 4], dtype=int32), 'rights': array([1, 3, 5], dtype=int32), 'dests': array([4, 5, 6], dtype=int32), 'words': array([252,  71, 253, 254], dtype=int32), 'labels': array([3, 1, 2], dtype=int32), 'leaf_labels': array([3, 2, 1, 2], dtype=int32)}

3.2 Definition of mini-batchable models
----------------------------------------

Recursive Neural Networks have two operations:

* Operation A : Computing an embedding vector at a leaf node.
* Operation B : Computing a hidden state vector at a branch node from hidden state vectors of two
  child nodes.

For each sample, we assign index to each node in returning order. If we traverse nodes in return order,
at first we visit leaf nodes and perform operation A. Later, we visit branch nodes and perform operation B.

This procedure can also be regarded as scanning a tree using a stack.
A stack is a last-in, first-out data structure, with two principal operations: a push operation to add data and a pop operation
to get the last pushed data.
For operation A, push the calculation result to the stack. For operation B, pop two data and push the new
calculation result again.

When we parallelize the above operation,
we must perform operation A on leaf nodes and operation B on branch nodes,
regardless whatever tree structures are included in mini-batched data.
It is quite difficult because the tree structure varies by samples. However, by using the stack,
we can precisely perform the above procedure by repeating processing without if-statements.
Therefore, parallelization is possible.

.. literalinclude:: ../../../examples/sentiment/train_recursive_minibatch.py
   :language: python
   :pyobject: ThinStackRecursiveNet
   :caption: train_recursive_minibatch.py

First ``for`` block calculates ``loss`` of leafs, and pushs the embedding vector ``es``
to the ``stack``. Second ``for`` block pops the hidden state vectors of its children,
calculates ``loss`` of nodes, and pushs the hidden state vector ``o`` again.

Most important point is that ``ThinStackRecursiveNet`` shares GPU memory for :class:`~chainer.Variable`\ s by
using thin-stack. It only allocate the whole memory, and reuse it when calculating forward and backward propagation.
Without careful implementation, backward propagation does not work because rewriting the memory may delete
neccesary information for backward propagation. However, with thin-stack, each write operation always refers
different index of the memory. So, it does not delete neccesary information for other read operations.

Let the sentence length be :math:`I`, and the number of dimensions of the hidden vector be :math:`D`,
the thin-stack can efficiently use the memory by using the matrix of :math:`(2I-1) \times D`.
In a normal stack, we need :math:`O(I^2 D)` space computation, whereas thin-stack requires :math:`O(ID)`.
This is realized by push operations ``thin_stack_set`` and pop operations ``thin_stack_get``.
For the memory allocation, we need the matrix of :math:`(2I-1) \times D` for each training example.
It means we need the matrix of :math:`B \times (2I-1) \times D` totally, where :math:`B`
is mini batch size. So, we allocate the memory for the stack by
``stack = self.xp.zeros((batch, maxlen * 2, self.n_units), 'f')``.

We define ``ThinStackSet`` and ``ThinStackGet`` which inherit ``chainer.Function``.
Because ``chainer.Function`` does not have internal states inside, it handles ``stack`` externally by passing
it as a function argument.


.. literalinclude:: ../../../examples/sentiment/thin_stack.py
   :language: python
   :pyobject: ThinStackSet
   :caption: thin_stack.py

``ThinStackSet`` is literally a function to set values on the thin-stack.
``inputs`` in ``forward`` and ``backward``  can be broken down like ``stack, indices, values = inputs``.
``thin_stack_set(stack, indices, values)`` sets stack elements. It behaves like
``stack[range(len(indices)), indices] = values``
and returns new ``stack``, where ``stack`` stores stack itself, ``indices`` store indices to be assigned,
and ``values`` store values to assign.

.. literalinclude:: ../../../examples/sentiment/thin_stack.py
   :language: python
   :pyobject: ThinStackGet
   :caption: thin_stack.py

``ThinStackGet`` is literally a function to retrieve values from the thin-stack.
``inputs`` in ``forward`` and ``backward`` can be broken down like ``stack, indices = inputs``.
``thin_stack_get(stack, indices)`` gets stack elements. It behaves like ``stack[range(len(indices)), indices]``.


.. literalinclude:: ../../../examples/sentiment/train_recursive_minibatch.py
   :language: python
   :start-after: test_data, args.batchsize, repeat=False, shuffle=False)
   :end-before: updater = training.StandardUpdater
   :caption: train_recursive_minibatch.py
   :dedent: 4

3.3 Preparation of Updater/Trainer and execution of training
----------------------------------------------------------------

Let's train with the new model ``ThinStackRecursiveNet``. Since we can now compute mini batches in parallel, we can see that training is faster.

.. literalinclude:: ../../../examples/sentiment/train_recursive_minibatch.py
   :language: python
   :start-after: optimizer.setup(model) 
   :end-before: __main__
   :caption: train_recursive_minibatch.py
   :dedent: 4

It should get much faster!

4. Reference
=============
* [1] `Socher, Richard; et al. "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank". <https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf>`_
* [2] `Parse tree <https://en.wikipedia.org/wiki/Parse_tree>`_
* [3] `The Stanford Parser: A statistical parser <https://nlp.stanford.edu/software/lex-parser.shtml>`_
* [4] `A Fast Unified Model for Parsing and Sentence Understanding <http://nlp.stanford.edu/pubs/bowman2016spinn.pdf>`_
